{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d5d2c0-cba8-404e-9bf3-71a218cae3cf",
   "metadata": {},
   "source": [
    "Packages that are being used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1305cf-12d5-46fe-a2c9-36fb71c5b3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1\n",
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a42fbfd-e3c2-43c2-bc12-f5f870a0b10a",
   "metadata": {},
   "source": [
    "- This notebook provides a brief overview of the data preparation and sampling procedures to get input data \"ready\" for an LLM\n",
    "- Understanding what the input data looks like is a great first step towards understanding how LLMs work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b2922-594d-4ff9-bd82-04f1ebdf41f5",
   "metadata": {},
   "source": [
    "<img src=\"./figures/01.png\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddbb984-8d23-40c5-bbfa-c3c379e7eec3",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 2.1 Tokenizing text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c90731-7dc9-4cd3-8c4a-488e33b48e80",
   "metadata": {},
   "source": [
    "- In this section, we tokenize text, which means breaking text into smaller units, such as individual words and punctuation characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09872fdb-9d4e-40c4-949d-52a01a43ec4b",
   "metadata": {},
   "source": [
    "<img src=\"figures/02.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cceaa18-833d-46b6-b211-b20c53902805",
   "metadata": {},
   "source": [
    "- Load raw text we want to work with\n",
    "- [The Verdict by Edith Wharton](https://en.wikisource.org/wiki/The_Verdict) is a public domain short story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a769e87-470a-48b9-8bdb-12841b416198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b971a46-ac03-4368-88ae-3f20279e8f4e",
   "metadata": {},
   "source": [
    "- The goal is to tokenize and embed this text for an LLM\n",
    "- Let's develop a simple tokenizer based on some simple sample text that we can then later apply to the text above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe9330-b587-4262-be9f-497a84ec0e8a",
   "metadata": {},
   "source": [
    "<img src=\"figures/03.png\" width=\"690px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa1687-2c08-485a-87cc-a93c2f9586d7",
   "metadata": {},
   "source": [
    "- The following regular expression will split on whitespaces and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "737dd5b0-9dbb-4a97-9ae4-3482c8c04be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 9235\n",
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '--', 'though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough', '--', 'so', ' ', 'it', ' ', 'was', ' ', 'no', ' ']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "print(\"Number of tokens:\", len(preprocessed))\n",
    "print(preprocessed[:38])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81896d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 1133\n",
      "['', '\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '--', '.', ':', ';', '?', 'A', 'Ah', 'Among', 'And', 'Are', 'Arrt', 'As', 'At', 'Be', 'Begin', 'Burlington', 'But', 'By', 'Carlo', 'Chicago', 'Claude', 'Come', 'Croft', 'Destroyed', 'Devonshire', 'Don', 'Dubarry', 'Emperors', 'Florence', 'For', 'Gallery', 'Gideon', 'Gisburn', 'Gisburns', 'Grafton', 'Greek', 'Grindle', 'Grindles', 'HAD', 'Had', 'Hang', 'Has', 'He', 'Her', 'Hermia', 'His', 'How', 'I', 'If', 'In', 'It', 'Jack', 'Jove', 'Just', 'Lord', 'Made', 'Miss', 'Money', 'Monte', 'Moon-dancers', 'Mr', 'Mrs', 'My', 'Never', 'No', 'Now', 'Nutley', 'Of', 'Oh', 'On', 'Once', 'Only', 'Or', 'Perhaps', 'Poor', 'Professional', 'Renaissance', 'Rickham', 'Riviera', 'Rome', 'Russian', 'Sevres', 'She', 'Stroud', 'Strouds', 'Suddenly', 'That', 'The', 'Then', 'There', 'They', 'This', 'Those', 'Though', 'Thwing', 'Thwings', 'To', 'Usually', 'Venetian', 'Victor', 'Was', 'We', 'Well', 'What', 'When', 'Why', 'Yes', 'You', '_', 'a', 'abdication', 'able', 'about', 'above', 'abruptly', 'absolute', 'absorbed', 'absurdity', 'academic', 'accuse', 'accustomed', 'across', 'activity', 'add', 'added', 'admirers', 'adopted', 'adulation', 'advance', 'aesthetic', 'affect', 'afraid', 'after', 'afterward', 'again', 'ago', 'ah', 'air', 'alive', 'all', 'almost', 'alone', 'along', 'always', 'am', 'amazement', 'amid', 'among', 'amplest', 'amusing', 'an', 'and', 'another', 'answer', 'answered', 'any', 'anything', 'anywhere', 'apparent', 'apparently', 'appearance', 'appeared', 'appointed', 'are', 'arm', 'arm-chair', 'arm-chairs', 'arms', 'art', 'articles', 'artist', 'as', 'aside', 'asked', 'at', 'atmosphere', 'atom', 'attack', 'attention', 'attitude', 'audacities', 'away', 'awful', 'axioms', 'azaleas', 'back', 'background', 'balance', 'balancing', 'balustraded', 'basking', 'bath-rooms', 'be', 'beaming', 'bean-stalk', 'bear', 'beard', 'beauty', 'became', 'because', 'becoming', 'bed', 'been', 'before', 'began', 'begun', 'behind', 'being', 'believed', 'beneath', 'bespoke', 'better', 'between', 'big', 'bits', 'bitterness', 'blocked', 'born', 'borne', 'boudoir', 'bravura', 'break', 'breaking', 'breathing', 'bric-a-brac', 'briefly', 'brings', 'bronzes', 'brought', 'brown', 'brush', 'bull', 'business', 'but', 'buying', 'by', 'called', 'came', 'can', 'canvas', 'canvases', 'cards', 'care', 'career', 'caught', 'central', 'chair', 'chap', 'characteristic', 'charming', 'cheap', 'check', 'cheeks', 'chest', 'chimney-piece', 'chucked', 'cigar', 'cigarette', 'cigars', 'circulation', 'circumstance', 'circus-clown', 'claimed', 'clasping', 'clear', 'cleverer', 'close', 'clue', 'coat', 'collapsed', 'colour', 'come', 'comfortable', 'coming', 'companion', 'compared', 'complex', 'confident', 'congesting', 'conjugal', 'constraint', 'consummate', 'contended', 'continued', 'corner', 'corrected', 'could', 'couldn', 'count', 'countenance', 'couple', 'course', 'covered', 'craft', 'cried', 'crossed', 'crowned', 'crumbled', 'cry', 'cured', 'curiosity', 'curious', 'current', 'curtains', 'd', 'dabble', 'damask', 'dark', 'dashed', 'day', 'days', 'dead', 'deadening', 'dear', 'deep', 'deerhound', 'degree', 'delicate', 'demand', 'denied', 'deploring', 'deprecating', 'deprecatingly', 'desire', 'destroyed', 'destruction', 'desultory', 'detail', 'diagnosis', 'did', 'didn', 'died', 'dim', 'dimmest', 'dingy', 'dining-room', 'disarming', 'discovery', 'discrimination', 'discussion', 'disdain', 'disdained', 'disease', 'disguised', 'display', 'dissatisfied', 'distinguished', 'distract', 'divert', 'do', 'doesn', 'doing', 'domestic', 'don', 'done', 'donkey', 'down', 'dozen', 'dragged', 'drawing-room', 'drawing-rooms', 'drawn', 'dress-closets', 'drew', 'dropped', 'each', 'earth', 'ease', 'easel', 'easy', 'echoed', 'economy', 'effect', 'effects', 'efforts', 'egregious', 'eighteenth-century', 'elbow', 'elegant', 'else', 'embarrassed', 'enabled', 'end', 'endless', 'enjoy', 'enlightenment', 'enough', 'ensuing', 'equally', 'equanimity', 'escape', 'established', 'etching', 'even', 'event', 'ever', 'everlasting', 'every', 'exasperated', 'except', 'excuse', 'excusing', 'existed', 'expected', 'exquisite', 'exquisitely', 'extenuation', 'exterminating', 'extracting', 'eye', 'eyebrows', 'eyes', 'face', 'faces', 'fact', 'faded', 'failed', 'failure', 'fair', 'faith', 'false', 'familiar', 'famille-verte', 'fancy', 'fashionable', 'fate', 'feather', 'feet', 'fell', 'fellow', 'felt', 'few', 'fewer', 'finality', 'find', 'fingers', 'first', 'fit', 'fitting', 'five', 'flash', 'flashed', 'florid', 'flowers', 'fluently', 'flung', 'follow', 'followed', 'fond', 'footstep', 'for', 'forced', 'forcing', 'forehead', 'foreign', 'foreseen', 'forgive', 'forgotten', 'form', 'formed', 'forming', 'forward', 'fostered', 'found', 'foundations', 'fragment', 'fragments', 'frame', 'frames', 'frequently', 'friend', 'from', 'full', 'fullest', 'furiously', 'furrowed', 'garlanded', 'garlands', 'gave', 'genial', 'genius', 'gesture', 'get', 'getting', 'give', 'given', 'glad', 'glanced', 'glimpse', 'gloried', 'glory', 'go', 'going', 'gone', 'good', 'good-breeding', 'good-humoured', 'got', 'grace', 'gradually', 'gray', 'grayish', 'great', 'greatest', 'greatness', 'grew', 'groping', 'growing', 'had', 'hadn', 'hair', 'half', 'half-light', 'half-mechanically', 'hall', 'hand', 'hands', 'handsome', 'hanging', 'happen', 'happened', 'hard', 'hardly', 'has', 'have', 'haven', 'having', 'he', 'head', 'hear', 'heard', 'heart', 'height', 'her', 'here', 'hermit', 'herself', 'hesitations', 'hide', 'high', 'him', 'himself', 'hint', 'his', 'history', 'holding', 'home', 'honour', 'hooded', 'hostess', 'hot-house', 'hour', 'hours', 'house', 'how', 'hung', 'husband', 'idea', 'idle', 'idling', 'if', 'immediately', 'in', 'incense', 'indifferent', 'inevitable', 'inevitably', 'inflexible', 'insensible', 'insignificant', 'instinctively', 'instructive', 'interesting', 'into', 'ironic', 'irony', 'irrelevance', 'irrevocable', 'is', 'it', 'its', 'itself', 'jardiniere', 'jealousy', 'just', 'keep', 'kept', 'kind', 'knees', 'knew', 'know', 'known', 'laid', 'lair', 'landing', 'language', 'last', 'late', 'later', 'latter', 'laugh', 'laughed', 'lay', 'leading', 'lean', 'learned', 'least', 'leathery', 'leave', 'led', 'left', 'leisure', 'lends', 'lent', 'let', 'lies', 'life', 'life-likeness', 'lift', 'lifted', 'light', 'lightly', 'like', 'liked', 'line', 'lines', 'lingered', 'lips', 'lit', 'little', 'live', 'll', 'loathing', 'long', 'longed', 'longer', 'look', 'looked', 'looking', 'lose', 'loss', 'lounging', 'lovely', 'lucky', 'lump', 'luncheon-table', 'luxury', 'lying', 'made', 'make', 'man', 'manage', 'managed', 'mantel-piece', 'marble', 'married', 'may', 'me', 'meant', 'mediocrity', 'medium', 'mentioned', 'mere', 'merely', 'met', 'might', 'mighty', 'millionaire', 'mine', 'minute', 'minutes', 'mirrors', 'modest', 'modesty', 'moment', 'money', 'monumental', 'mood', 'morbidly', 'more', 'most', 'mourn', 'mourned', 'moustache', 'moved', 'much', 'muddling', 'multiplied', 'murmur', 'muscles', 'must', 'my', 'myself', 'mysterious', 'naive', 'near', 'nearly', 'negatived', 'nervous', 'nervousness', 'neutral', 'never', 'next', 'no', 'none', 'not', 'note', 'nothing', 'now', 'nymphs', 'oak', 'obituary', 'object', 'objects', 'occurred', 'oddly', 'of', 'off', 'often', 'oh', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'open', 'or', 'other', 'our', 'ourselves', 'out', 'outline', 'oval', 'over', 'own', 'packed', 'paid', 'paint', 'painted', 'painter', 'painting', 'pale', 'paled', 'palm-trees', 'panel', 'panelling', 'pardonable', 'pardoned', 'part', 'passages', 'passing', 'past', 'pastels', 'pathos', 'patient', 'people', 'perceptible', 'perfect', 'persistence', 'persuasively', 'phrase', 'picture', 'pictures', 'pines', 'pink', 'place', 'placed', 'plain', 'platitudes', 'pleased', 'pockets', 'point', 'poised', 'poor', 'portrait', 'posing', 'possessed', 'poverty', 'predicted', 'preliminary', 'presenting', 'prestidigitation', 'pretty', 'previous', 'price', 'pride', 'princely', 'prism', 'problem', 'proclaiming', 'prodigious', 'profusion', 'protest', 'prove', 'public', 'purblind', 'purely', 'pushed', 'put', 'qualities', 'quality', 'queerly', 'question', 'quickly', 'quietly', 'quite', 'quote', 'rain', 'raised', 'random', 'rather', 're', 'real', 'really', 'reared', 'reason', 'reassurance', 'recovering', 'recreated', 'reflected', 'reflection', 'regrets', 'relatively', 'remained', 'remember', 'reminded', 'repeating', 'represented', 'reproduction', 'resented', 'resolve', 'resources', 'rest', 'rich', 'ridiculous', 'robbed', 'romantic', 'room', 'rose', 'rs', 'rule', 'run', 's', 'said', 'same', 'satisfaction', 'savour', 'saw', 'say', 'saying', 'says', 'scorn', 'scornful', 'secret', 'see', 'seemed', 'seen', 'self-confident', 'send', 'sensation', 'sensitive', 'sent', 'serious', 'set', 'sex', 'shade', 'shaking', 'shall', 'she', 'shirked', 'short', 'should', 'shoulder', 'shoulders', 'show', 'showed', 'showy', 'shrug', 'shrugged', 'sight', 'sign', 'silent', 'silver', 'similar', 'simpleton', 'simplifications', 'simply', 'since', 'single', 'sitter', 'sitters', 'sketch', 'skill', 'slight', 'slightly', 'slowly', 'small', 'smile', 'smiling', 'sneer', 'so', 'solace', 'some', 'somebody', 'something', 'spacious', 'spaniel', 'speaking-tubes', 'speculations', 'spite', 'splash', 'square', 'stairs', 'stammer', 'stand', 'standing', 'started', 'stay', 'still', 'stocked', 'stood', 'stopped', 'stopping', 'straddling', 'straight', 'strain', 'straining', 'strange', 'straw', 'stream', 'stroke', 'strokes', 'strolled', 'strongest', 'strongly', 'struck', 'studio', 'stuff', 'subject', 'substantial', 'suburban', 'such', 'suddenly', 'suffered', 'sugar', 'suggested', 'sunburn', 'sunburnt', 'sunlit', 'superb', 'sure', 'surest', 'surface', 'surprise', 'surprised', 'surrounded', 'suspected', 'sweetly', 'sweetness', 'swelling', 'swept', 'swum', 't', 'table', 'take', 'taken', 'talking', 'tea', 'tears', 'technicalities', 'technique', 'tell', 'tells', 'tempting', 'terra-cotta', 'terrace', 'terraces', 'terribly', 'than', 'that', 'the', 'their', 'them', 'then', 'there', 'therefore', 'they', 'thin', 'thing', 'things', 'think', 'this', 'thither', 'those', 'though', 'thought', 'three', 'threshold', 'threw', 'through', 'throwing', 'tie', 'till', 'time', 'timorously', 'tinge', 'tips', 'tired', 'to', 'told', 'tone', 'tones', 'too', 'took', 'tottering', 'touched', 'toward', 'trace', 'trade', 'transmute', 'traps', 'travelled', 'tribute', 'tributes', 'tricks', 'tried', 'trouser-presses', 'true', 'truth', 'turned', 'twenty', 'twenty-four', 'twice', 'twirling', 'unaccountable', 'uncertain', 'under', 'underlay', 'underneath', 'understand', 'unexpected', 'untouched', 'unusual', 'up', 'up-stream', 'upon', 'upset', 'upstairs', 'us', 'used', 'usual', 'value', 'varnishing', 'vases', 've', 'veins', 'velveteen', 'very', 'villa', 'vindicated', 'virtuosity', 'vista', 'vocation', 'voice', 'wall', 'wander', 'want', 'wanted', 'wants', 'was', 'wasn', 'watched', 'watching', 'water-colour', 'waves', 'way', 'weekly', 'weeks', 'welcome', 'went', 'were', 'what', 'when', 'whenever', 'where', 'which', 'while', 'white', 'white-panelled', 'who', 'whole', 'whom', 'why', 'wide', 'widow', 'wife', 'wild', 'wincing', 'window-curtains', 'wish', 'with', 'without', 'wits', 'woman', 'women', 'won', 'wonder', 'wondered', 'word', 'work', 'working', 'worth', 'would', 'wouldn', 'year', 'years', 'yellow', 'yet', 'you', 'younger', 'your', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = set(preprocessed)\n",
    "print(\"Number of unique tokens:\", len(unique_tokens))\n",
    "print(sorted(list(unique_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ce8fe-3a07-4f2a-90f1-a0321ce3a231",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 2.2 Converting tokens into token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5204973-f414-4c0d-87b0-cfec1f06e6ff",
   "metadata": {},
   "source": [
    "- Next, we convert the text tokens into token IDs that we can process via embedding layers later\n",
    "- For this we first need to build a vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b041d-f739-43b8-bd81-0443ae3a7f8d",
   "metadata": {},
   "source": [
    "<img src=\"figures/04.png\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeade64-037b-4b59-9039-d3b000ef8886",
   "metadata": {},
   "source": [
    "- The vocabulary contains the unique words in the input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fdf0533-5ab6-42a5-83fa-a3b045de6396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(unique_tokens)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77d00d96-881f-4691-bb03-84fec2a75a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0, 'cheeks': 1, 'romantic': 2, 'the': 3, 'come': 4, 'near': 5, 'often': 6, 'lean': 7, 'azaleas': 8, 'leathery': 9, 'mourn': 10, 'idle': 11, 'three': 12, 'most': 13, 'without': 14, 'just': 15, 'none': 16, 'reflection': 17, 'brings': 18, 'profusion': 19, 'fond': 20, 'equally': 21, 'fitting': 22, 'Only': 23, 'disdain': 24, 'wondered': 25, 'faith': 26, 'check': 27, 'Stroud': 28, 'patient': 29, 'underlay': 30, 'himself': 31, 'strolled': 32, 'pardonable': 33, 'might': 34, 'Lord': 35, 'reproduction': 36, 'seemed': 37, 'secret': 38, 'florid': 39, 'moment': 40, 'apparently': 41, 'suburban': 42, 'under': 43, 'grew': 44, 'failed': 45, 'detail': 46, 'half': 47, 'seen': 48, 'insensible': 49, 'minutes': 50, 'rest': 51, 'cigars': 52, 'strokes': 53, 'being': 54, 'pathos': 55, 'nothing': 56, 'rain': 57, 'tinge': 58, 'a': 59, 'way': 60, 'every': 61, 'over': 62, 'dimmest': 63, 'leading': 64, 'wife': 65, 'rs': 66, 'frame': 67, 'done': 68, 'attitude': 69, 'myself': 70, 'trade': 71, 'panelling': 72, 'stay': 73, 'chap': 74, 'only': 75, 'dim': 76, '.': 77, 'strain': 78, 'made': 79, 'objects': 80, 'enlightenment': 81, 'Monte': 82, 'savour': 83, 'lies': 84, 'idling': 85, 'Riviera': 86, 'because': 87, 'perceptible': 88, 'garlanded': 89, 'painting': 90, 'enabled': 91, 'diagnosis': 92, 'afterward': 93, 'amplest': 94, 'stand': 95, 't': 96, 'lips': 97, 'landing': 98, 'lent': 99, 'house': 100, 'up-stream': 101, 'tones': 102, 'Jove': 103, 'instructive': 104, 'hadn': 105, 'Had': 106, 'mere': 107, 'gone': 108, 'no': 109, 'foundations': 110, 'crowned': 111, 'virtuosity': 112, 'abruptly': 113, 'surrounded': 114, 'distinguished': 115, 'why': 116, 'raised': 117, 'indifferent': 118, 'twenty-four': 119, 'hooded': 120, 'proclaiming': 121, 'villa': 122, 'fell': 123, 'pictures': 124, 'Has': 125, 'consummate': 126, 'background': 127, 'manage': 128, 'At': 129, 'things': 130, 'threshold': 131, 'stopping': 132, 'Was': 133, 'swum': 134, 'showed': 135, 'princely': 136, 'destruction': 137, 'known': 138, 'efforts': 139, 'managed': 140, 'Among': 141, 'doing': 142, 'history': 143, 'break': 144, 'attention': 145, 'exquisitely': 146, 'quite': 147, 'mirrors': 148, 'straddling': 149, 'woman': 150, 'thought': 151, 'dark': 152, 'We': 153, 'dining-room': 154, 'posing': 155, 'upset': 156, 'prism': 157, 'economy': 158, 'day': 159, 'arm-chairs': 160, 'back': 161, 'afraid': 162, 'form': 163, 'splash': 164, 'about': 165, 'packed': 166, 'to': 167, 'cry': 168, '--': 169, 'show': 170, 'hostess': 171, 'straining': 172, 'neutral': 173, 'circus-clown': 174, 'never': 175, 'brought': 176, 'paid': 177, 'forgotten': 178, 'fate': 179, 'interesting': 180, 'sweetly': 181, 'Gisburns': 182, 'sitter': 183, 'breathing': 184, 'Greek': 185, '\\n': 186, 'Of': 187, 'passing': 188, 'painter': 189, 'Are': 190, 'reared': 191, 'language': 192, 'hesitations': 193, 'stream': 194, 'deep': 195, 'forgive': 196, 'won': 197, 'forced': 198, 'buying': 199, 'lair': 200, 'least': 201, 'absolute': 202, 'hours': 203, 'sight': 204, 'shrug': 205, 'becoming': 206, 'jealousy': 207, 'gray': 208, 'balustraded': 209, 'display': 210, 'spacious': 211, 'deprecatingly': 212, 'Yes': 213, 'dabble': 214, 'awful': 215, 'There': 216, 'care': 217, 'It': 218, 'crumbled': 219, 'irrevocable': 220, 'borne': 221, 'tie': 222, 'famille-verte': 223, 'whenever': 224, 'covered': 225, 'somebody': 226, 'took': 227, 'so': 228, 'enjoy': 229, 'twenty': 230, 'her': 231, 'silver': 232, 'balance': 233, 'companion': 234, 'up': 235, 'simply': 236, 'surest': 237, 'moustache': 238, 'simplifications': 239, 'A': 240, 'nervousness': 241, 'still': 242, 'unexpected': 243, 'even': 244, 'wild': 245, 'recovering': 246, 'slowly': 247, 'echoed': 248, 'glory': 249, 'lightly': 250, 'caught': 251, 'his': 252, 'who': 253, 'old': 254, 'born': 255, 'couldn': 256, 'pleased': 257, 'stairs': 258, 'bespoke': 259, 'swelling': 260, 'end': 261, 'added': 262, 'big': 263, 'sunlit': 264, 'sunburnt': 265, 'gradually': 266, 'murmur': 267, 'asked': 268, 'Rome': 269, 'else': 270, 'untouched': 271, 'accuse': 272, 'oak': 273, 'Moon-dancers': 274, 'put': 275, 'widow': 276, 'obituary': 277, 'axioms': 278, 'smiling': 279, 'stuff': 280, 'looking': 281, 'brown': 282, 'appeared': 283, 'longed': 284, 'greatness': 285, 'previous': 286, 'standing': 287, 'alive': 288, 'laid': 289, 'affect': 290, 'quietly': 291, 'alone': 292, 'claimed': 293, 'monumental': 294, 'traps': 295, 'became': 296, 'attack': 297, 'problem': 298, 'lines': 299, 'Perhaps': 300, 'very': 301, 'prove': 302, 'beaming': 303, 'Dubarry': 304, 'tone': 305, 're': 306, 'ironic': 307, 'air': 308, 'donkey': 309, 'straw': 310, 'Begin': 311, 'lump': 312, 'degree': 313, 'want': 314, 'mysterious': 315, 'Usually': 316, 'am': 317, 'didn': 318, 'Russian': 319, 'trouser-presses': 320, 'us': 321, 'therefore': 322, 'those': 323, 'that': 324, 'tempting': 325, 'small': 326, 'down': 327, 'before': 328, 'real': 329, 'died': 330, 'surprised': 331, 'worth': 332, 'morbidly': 333, 'chimney-piece': 334, 'place': 335, 'circulation': 336, 'later': 337, 'pines': 338, 'reminded': 339, 'twirling': 340, ':': 341, 'Mrs': 342, 'run': 343, 'one': 344, 'other': 345, 'Hang': 346, 'shaking': 347, 'divert': 348, 'presenting': 349, 'sitters': 350, 'like': 351, 'deadening': 352, 'conjugal': 353, 'effect': 354, 'hermit': 355, 'purely': 356, 'part': 357, 'note': 358, 'window-curtains': 359, 'excuse': 360, 'Burlington': 361, 'be': 362, 'glimpse': 363, 'man': 364, 'has': 365, 'Well': 366, 'single': 367, 'stammer': 368, 'said': 369, 'resented': 370, 'aside': 371, 'chest': 372, 'cured': 373, 'him': 374, 'fullest': 375, 'frequently': 376, 'here': 377, 'there': 378, 'hardly': 379, 'few': 380, 'disarming': 381, 'Destroyed': 382, 'forward': 383, 'Once': 384, 'congesting': 385, 'irony': 386, 'continued': 387, 'say': 388, 'solace': 389, 'sex': 390, 'was': 391, 'corner': 392, 'You': 393, 'bear': 394, 'bits': 395, 'surprise': 396, 'me': 397, 'younger': 398, 'event': 399, 'deprecating': 400, 'heart': 401, 'absorbed': 402, 'oddly': 403, 'Grindle': 404, 'simpleton': 405, 'point': 406, 'full': 407, '!': 408, 'everlasting': 409, 'dropped': 410, 'kept': 411, 'five': 412, 'idea': 413, 'rather': 414, 'make': 415, 'Now': 416, 'ourselves': 417, 'Jack': 418, 'heard': 419, 'absurdity': 420, 'marble': 421, 'When': 422, 'dress-closets': 423, 'phrase': 424, 'As': 425, 'lovely': 426, 'activity': 427, 'distract': 428, 'haven': 429, 'toward': 430, 'at': 431, 'touched': 432, 'Devonshire': 433, 'For': 434, 'finality': 435, 'fashionable': 436, 'substantial': 437, 'Never': 438, 'curious': 439, 'queerly': 440, 'Grindles': 441, 'bath-rooms': 442, 'looked': 443, 'art': 444, 'saying': 445, 've': 446, 'muddling': 447, 'this': 448, 'lit': 449, 'threw': 450, 'wouldn': 451, 'footstep': 452, 'white-panelled': 453, 'destroyed': 454, 'wish': 455, 'short': 456, 'familiar': 457, 'among': 458, 'first': 459, 'furrowed': 460, 'behind': 461, 'furiously': 462, 'They': 463, 'lose': 464, 'discrimination': 465, 'medium': 466, 'oh': 467, 'long': 468, 'spite': 469, 'have': 470, 'dissatisfied': 471, 'trace': 472, 'usual': 473, 'give': 474, 'Croft': 475, 'word': 476, 'similar': 477, 'random': 478, 'fit': 479, 'd': 480, 'canvases': 481, 'pale': 482, 'bed': 483, 'sketch': 484, 'artist': 485, 'feet': 486, 'thin': 487, 'technique': 488, 'occurred': 489, 'sneer': 490, 'domestic': 491, '(': 492, 'thither': 493, 'high': 494, 'holding': 495, 'quality': 496, 'told': 497, 'scorn': 498, 'Rickham': 499, 'canvas': 500, 'foreseen': 501, 'resources': 502, 'inevitable': 503, 'money': 504, 'remained': 505, 'square': 506, 'genial': 507, 'sure': 508, 'suggested': 509, 'Hermia': 510, 'elbow': 511, 'false': 512, 'honour': 513, 'delicate': 514, 'yellow': 515, 'nearly': 516, 'throwing': 517, 'open': 518, 'bean-stalk': 519, 'faces': 520, 'always': 521, 'll': 522, 'dragged': 523, 'atmosphere': 524, 'resolve': 525, 'then': 526, 'robbed': 527, 'easel': 528, 'Then': 529, 'good': 530, 'sent': 531, 'His': 532, 'from': 533, 'couple': 534, 'may': 535, 'did': 536, 'satisfaction': 537, 'equanimity': 538, 'transmute': 539, 'welcome': 540, 'wonder': 541, 'pockets': 542, 'Grafton': 543, 'shoulder': 544, 'but': 545, 'And': 546, 'Poor': 547, 'qualities': 548, 'cigar': 549, 'onto': 550, 'you': 551, 'get': 552, 'let': 553, 'send': 554, 'ease': 555, 'outline': 556, 'tell': 557, 'pardoned': 558, 'hair': 559, 'of': 560, 'gave': 561, 'rule': 562, 'forcing': 563, 'complex': 564, 'balancing': 565, 'knees': 566, 'abdication': 567, 'velveteen': 568, 'really': 569, 'went': 570, 'began': 571, 'turned': 572, 'meant': 573, 'forming': 574, 'life': 575, 'eighteenth-century': 576, 'appointed': 577, 'speculations': 578, 'drawing-room': 579, 'happen': 580, 'disdained': 581, 'anything': 582, 'good-humoured': 583, 'quickly': 584, 'speaking-tubes': 585, ';': 586, 'mantel-piece': 587, 'panel': 588, 'protest': 589, 'liked': 590, 'modesty': 591, 'boudoir': 592, 'wanted': 593, 'Strouds': 594, 'technicalities': 595, ' ': 596, 'same': 597, 'what': 598, 'kind': 599, 'discussion': 600, 'fostered': 601, 'value': 602, \"'\": 603, 'exquisite': 604, 'followed': 605, 'muscles': 606, 'thing': 607, 'fingers': 608, 'persistence': 609, 'bull': 610, 'height': 611, 'exterminating': 612, 'vindicated': 613, 'Arrt': 614, 'tired': 615, 'better': 616, 'once': 617, '_': 618, 'answered': 619, 'lifted': 620, 'any': 621, 'believed': 622, 'how': 623, 'and': 624, 'slightly': 625, 'forehead': 626, 'coming': 627, 'immediately': 628, 'such': 629, 'upstairs': 630, 'vocation': 631, 'head': 632, 'gesture': 633, 'earth': 634, 'wits': 635, 'stroke': 636, 'learned': 637, 'inevitably': 638, 'must': 639, 'your': 640, 'luxury': 641, 'mine': 642, 'crossed': 643, 'passages': 644, 'look': 645, 'poised': 646, 'multiplied': 647, 'Miss': 648, 'fact': 649, 'sign': 650, 'Oh': 651, 'escape': 652, 'face': 653, 'days': 654, 'across': 655, 'palm-trees': 656, 'dear': 657, 'dozen': 658, 'started': 659, 'repeating': 660, 'terrace': 661, 'He': 662, 'HAD': 663, 'wasn': 664, 'effects': 665, 'travelled': 666, 'fragment': 667, 'clasping': 668, 'go': 669, 'spaniel': 670, 'with': 671, 'audacities': 672, 'where': 673, 'swept': 674, 'slight': 675, 'bravura': 676, 'beauty': 677, 'academic': 678, 'keep': 679, 'between': 680, 'Suddenly': 681, 'arms': 682, 'waves': 683, 'Money': 684, 'upon': 685, 'through': 686, 'appearance': 687, 'My': 688, 'painted': 689, 'plain': 690, 'Claude': 691, 'contended': 692, 'coat': 693, 'prestidigitation': 694, 'bric-a-brac': 695, 'In': 696, 'portrait': 697, 'fewer': 698, 'existed': 699, 'central': 700, 'strongly': 701, 'mood': 702, 'Just': 703, 'met': 704, 'atom': 705, 'when': 706, 'Emperors': 707, 'insignificant': 708, 'hanging': 709, 'talking': 710, 'clear': 711, 'predicted': 712, 'nervous': 713, 'reassurance': 714, 'object': 715, 'left': 716, 'desultory': 717, 'off': 718, 'strongest': 719, 'whom': 720, 'curtains': 721, 'weekly': 722, 'Victor': 723, 'To': 724, 'foreign': 725, 'disease': 726, 'negatived': 727, 'Nutley': 728, 'ones': 729, 'studio': 730, 'by': 731, 'stocked': 732, 'lying': 733, 'comfortable': 734, 'married': 735, 'reflected': 736, ',': 737, 'public': 738, 'aesthetic': 739, 'denied': 740, 'friend': 741, 'corrected': 742, 'hands': 743, 'suspected': 744, 'leisure': 745, 'hand': 746, 'which': 747, 'now': 748, 'line': 749, 'tried': 750, 'curiosity': 751, 'Sevres': 752, 'says': 753, 'luncheon-table': 754, 'pride': 755, 's': 756, 'mourned': 757, 'could': 758, 'dead': 759, 'or': 760, 'beard': 761, 'eyebrows': 762, 'time': 763, 'remember': 764, 'pushed': 765, 'advance': 766, 'into': 767, 'extenuation': 768, 'Or': 769, 'instinctively': 770, 'prodigious': 771, 'I': 772, 'mighty': 773, 'incense': 774, 'underneath': 775, 'been': 776, 'question': 777, 'ensuing': 778, 'longer': 779, 'amid': 780, 'came': 781, 'bronzes': 782, 'varnishing': 783, 'all': 784, 'confident': 785, 'deerhound': 786, 'suffered': 787, 'blocked': 788, 'struck': 789, 'naive': 790, 'adopted': 791, 'characteristic': 792, 'saw': 793, 'exasperated': 794, 'Ah': 795, 'subject': 796, 'garlands': 797, 'hour': 798, 'used': 799, 'frames': 800, 'sensitive': 801, 'damask': 802, 'room': 803, 'sensation': 804, 'unusual': 805, 'amusing': 806, 'articles': 807, 'itself': 808, 'etching': 809, 'yet': 810, 'take': 811, 'not': 812, 'desire': 813, 'stopped': 814, 'on': 815, 'uncertain': 816, 'their': 817, 'hear': 818, 'paled': 819, 'wants': 820, 'half-light': 821, 'loss': 822, 'watched': 823, 'got': 824, 'pink': 825, 'in': 826, 'some': 827, 'ridiculous': 828, 'ever': 829, 'arm-chair': 830, 'grayish': 831, 'shirked': 832, 'pastels': 833, 'superb': 834, 'platitudes': 835, 'suddenly': 836, 'represented': 837, 'embarrassed': 838, 'as': 839, 'understand': 840, 'cigarette': 841, 'flashed': 842, 'husband': 843, 'formed': 844, 'flowers': 845, 'career': 846, 'glad': 847, 'vases': 848, 'What': 849, 'nymphs': 850, 'business': 851, 'growing': 852, 'flash': 853, 'taken': 854, 'herself': 855, 'skill': 856, 'work': 857, 'price': 858, 'for': 859, 'Thwing': 860, 'laugh': 861, 'happened': 862, 'If': 863, 'since': 864, 'recreated': 865, 'Renaissance': 866, 'if': 867, 'know': 868, 'terribly': 869, 'don': 870, 'course': 871, 'inflexible': 872, 'rich': 873, 'Those': 874, 'along': 875, 'drew': 876, 'grace': 877, 'led': 878, 'it': 879, 'That': 880, 'white': 881, 'wall': 882, 'its': 883, 'regrets': 884, 'having': 885, 'apparent': 886, 'fellow': 887, 'shall': 888, 'countenance': 889, 'relatively': 890, 'each': 891, 'How': 892, 'life-likeness': 893, 'add': 894, 'groping': 895, 'Gideon': 896, 'lounging': 897, 'again': 898, 'millionaire': 899, 'answer': 900, 'laughed': 901, 'able': 902, 'is': 903, 'should': 904, 'showy': 905, 'craft': 906, 'cleverer': 907, 'more': 908, 'easy': 909, 'disguised': 910, 'weeks': 911, 'timorously': 912, 'compared': 913, 'Why': 914, 'tips': 915, 'shoulders': 916, 'endless': 917, 'Come': 918, 'they': 919, 'enough': 920, 'straight': 921, 'perfect': 922, 'constraint': 923, 'persuasively': 924, 'twice': 925, 'accustomed': 926, 'hall': 927, 'rose': 928, 'terraces': 929, 'tells': 930, 'fancy': 931, 'veins': 932, 'live': 933, 'lingered': 934, 'irrelevance': 935, 'found': 936, 'much': 937, 'out': 938, ')': 939, 'she': 940, 'fluently': 941, 'eyes': 942, 'think': 943, 'watching': 944, 'dingy': 945, 'almost': 946, 'past': 947, 'Made': 948, 'an': 949, 'The': 950, 'genius': 951, 'stood': 952, 'merely': 953, 'while': 954, 'light': 955, 'loathing': 956, 'Thwings': 957, 'But': 958, 'breaking': 959, 'Though': 960, 'follow': 961, 'are': 962, 'basking': 963, 'anywhere': 964, 'except': 965, 'By': 966, 'circumstance': 967, 'do': 968, 'adulation': 969, 'tricks': 970, 'excusing': 971, 'self-confident': 972, 'handsome': 973, 'faded': 974, 'flung': 975, 'though': 976, 'away': 977, '?': 978, 'jardiniere': 979, '\"': 980, 'too': 981, 'find': 982, 'failure': 983, 'hot-house': 984, 'late': 985, 'admirers': 986, 'Her': 987, 'were': 988, 'mentioned': 989, 'lends': 990, 'wide': 991, 'unaccountable': 992, 'after': 993, 'chair': 994, 'This': 995, 'glanced': 996, 'extracting': 997, 'doesn': 998, 'Professional': 999, 'called': 1000, 'oval': 1001, 'She': 1002, 'fair': 1003, 'dashed': 1004, 'scornful': 1005, 'paint': 1006, 'colour': 1007, 'women': 1008, 'hung': 1009, 'preliminary': 1010, 'hard': 1011, 'till': 1012, 'placed': 1013, 'Venetian': 1014, 'latter': 1015, 'whole': 1016, 'true': 1017, 'brush': 1018, 'ah': 1019, 'fragments': 1020, 'own': 1021, 'wander': 1022, 'begun': 1023, 'chucked': 1024, 'hide': 1025, 'sunburn': 1026, 'my': 1027, 'close': 1028, 'moved': 1029, 'yourself': 1030, 'mediocrity': 1031, 'No': 1032, 'egregious': 1033, 'pretty': 1034, 'terra-cotta': 1035, 'cards': 1036, 'greatest': 1037, 'see': 1038, 'tributes': 1039, 'serious': 1040, 'Gisburn': 1041, 'sugar': 1042, 'Florence': 1043, 'he': 1044, 'quote': 1045, 'year': 1046, 'eye': 1047, 'hint': 1048, 'strange': 1049, 'lift': 1050, 'had': 1051, 'demand': 1052, 'discovery': 1053, 'getting': 1054, 'cheap': 1055, 'above': 1056, 'would': 1057, 'gloried': 1058, 'next': 1059, 'little': 1060, 'On': 1061, 'voice': 1062, 'Gallery': 1063, 'another': 1064, 'established': 1065, 'them': 1066, 'table': 1067, 'amazement': 1068, 'something': 1069, 'home': 1070, 'elegant': 1071, 'tears': 1072, 'lucky': 1073, 'half-mechanically': 1074, 'Carlo': 1075, 'count': 1076, 'great': 1077, 'beneath': 1078, 'Be': 1079, 'bitterness': 1080, 'last': 1081, 'tribute': 1082, 'surface': 1083, 'deploring': 1084, 'people': 1085, 'briefly': 1086, 'knew': 1087, 'shrugged': 1088, 'felt': 1089, 'truth': 1090, 'Chicago': 1091, 'purblind': 1092, 'years': 1093, 'picture': 1094, 'Don': 1095, 'water-colour': 1096, 'good-breeding': 1097, 'possessed': 1098, 'drawn': 1099, 'clue': 1100, 'charming': 1101, 'going': 1102, 'drawing-rooms': 1103, 'can': 1104, 'silent': 1105, 'modest': 1106, 'shade': 1107, 'vista': 1108, 'sweetness': 1109, 'arm': 1110, 'cried': 1111, 'tottering': 1112, 'minute': 1113, 'reason': 1114, 'smile': 1115, 'poor': 1116, 'lay': 1117, 'poverty': 1118, 'expected': 1119, 'collapsed': 1120, 'Mr': 1121, 'set': 1122, 'our': 1123, 'current': 1124, 'ago': 1125, 'than': 1126, 'feather': 1127, 'leave': 1128, 'tea': 1129, 'given': 1130, 'wincing': 1131, 'working': 1132}\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(unique_tokens)}\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd1f81-3a8f-4dd9-9dd6-e75f32dacbe3",
   "metadata": {},
   "source": [
    "- Below are the first 50 entries in this vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1c5de4a-aa4e-4aec-b532-10bb364039d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', 0)\n",
      "('cheeks', 1)\n",
      "('romantic', 2)\n",
      "('the', 3)\n",
      "('come', 4)\n",
      "('near', 5)\n",
      "('often', 6)\n",
      "('lean', 7)\n",
      "('azaleas', 8)\n",
      "('leathery', 9)\n",
      "('mourn', 10)\n",
      "('idle', 11)\n",
      "('three', 12)\n",
      "('most', 13)\n",
      "('without', 14)\n",
      "('just', 15)\n",
      "('none', 16)\n",
      "('reflection', 17)\n",
      "('brings', 18)\n",
      "('profusion', 19)\n",
      "('fond', 20)\n",
      "('equally', 21)\n",
      "('fitting', 22)\n",
      "('Only', 23)\n",
      "('disdain', 24)\n",
      "('wondered', 25)\n",
      "('faith', 26)\n",
      "('check', 27)\n",
      "('Stroud', 28)\n",
      "('patient', 29)\n",
      "('underlay', 30)\n",
      "('himself', 31)\n",
      "('strolled', 32)\n",
      "('pardonable', 33)\n",
      "('might', 34)\n",
      "('Lord', 35)\n",
      "('reproduction', 36)\n",
      "('seemed', 37)\n",
      "('secret', 38)\n",
      "('florid', 39)\n",
      "('moment', 40)\n",
      "('apparently', 41)\n",
      "('suburban', 42)\n",
      "('under', 43)\n",
      "('grew', 44)\n",
      "('failed', 45)\n",
      "('detail', 46)\n",
      "('half', 47)\n",
      "('seen', 48)\n",
      "('insensible', 49)\n"
     ]
    }
   ],
   "source": [
    "vocab_items = list(vocab.items())\n",
    "i = 0\n",
    "while i<50:\n",
    "    print(vocab_items[i])\n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1dc314-351b-476a-9459-0ec9ddc29b19",
   "metadata": {},
   "source": [
    "- Below, we illustrate the tokenization of a short sample text using a small vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67407a9f-0202-4e7c-9ed7-1b3154191ebc",
   "metadata": {},
   "source": [
    "<img src=\"figures/05.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e569647-2589-4c9d-9a5c-aef1c88a0a9a",
   "metadata": {},
   "source": [
    "- Let's now put it all together into a tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f531bf46-7c25-4ef8-bff8-0d27518676d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee7a1e5-b54f-4ca1-87ef-3d663c4ee1e7",
   "metadata": {},
   "source": [
    "- The `encode` function turns text into token IDs\n",
    "- The `decode` function turns token IDs back into text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc21d347-ec03-4823-b3d4-9d686e495617",
   "metadata": {},
   "source": [
    "<img src=\"figures/06.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2950a94-6b0d-474e-8ed0-66d0c3c1a95c",
   "metadata": {},
   "source": [
    "- We can use the tokenizer to encode (that is, tokenize) texts into integers\n",
    "- These integers can then be embedded (later) as input of/for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "647364ec-7995-4654-9b4a-7607ccf5f1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[218, 603, 756, 3, 1081, 1044, 689, 737, 551, 868, 737, 342, 77, 1041, 369, 671, 33, 755, 77]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"It's the last he painted, you know, Mrs. Gisburn said with pardonable pride.\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "\n",
    "# print(SimpleTokenizerV1(vocab).encode(text)) another way to do the above task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3201706e-a487-4b60-b99d-5765865f29a0",
   "metadata": {},
   "source": [
    "- We can decode the integers back into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01d8c8fb-432d-4a49-b332-99f23b233746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It' s the last he painted, you know, Mrs. Gisburn said with pardonable pride.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ba34b-170f-4e71-939b-77aabb776f14",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 2.3 BytePair encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309494c-79cf-4a2d-bc28-a94d602f050e",
   "metadata": {},
   "source": [
    "- GPT-2 used BytePair encoding (BPE) as its tokenizer\n",
    "- it allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n",
    "- For instance, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges\n",
    "- The original BPE tokenizer can be found here: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
    "- In this lecture, we are using the BPE tokenizer from OpenAI's open-source [tiktoken](https://github.com/openai/tiktoken) library, which implements its core algorithms in Rust to improve computational performance\n",
    "- (Based on an analysis [here](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/02_bonus_bytepair-encoder/compare-bpe-tiktoken.ipynb), I found that `tiktoken` is approx. 3x faster than the original tokenizer and 6x faster than an equivalent tokenizer in Hugging Face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1d41f-934b-4bf4-8184-54394a257a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48967a77-7d17-42bf-9e92-fc619d63a59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ad3312f-a5f7-4efc-9d7d-8ea09d7b5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff2cd85-7cfb-4325-b390-219938589428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "# The <|endoftext|> token is equal to 50256 which is the total no. of tokens in the GPT2 model BPE Tokenizer\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f64bb639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, 1530, 45429]\n",
      "K\n",
      "ush\n",
      "agra\n"
     ]
    }
   ],
   "source": [
    "# This BPE Tokenizer can even hanle unknown words by converting them into subwords and then into tokens\n",
    "print(tokenizer.encode(\"Kushagra\")) \n",
    "print(tokenizer.decode([42]))\n",
    "print(tokenizer.decode([1530]))\n",
    "print(tokenizer.decode([45429]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d26a48bb-f82e-41a8-a955-a1c9cf9d50ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2e7b4-6a22-42aa-8e4d-901f06378d4a",
   "metadata": {},
   "source": [
    "- BPE tokenizers break down unknown words into subwords and individual characters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c082d41f-33d7-4827-97d8-993d5a84bb3c",
   "metadata": {},
   "source": [
    "<img src=\"figures/07.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0beb27ee-1156-457c-839e-eebb48d94d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33901, 86, 343, 86, 220, 959]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Akwirw ier\", allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbd7c0d-70f8-4386-a114-907e96c950b0",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 2.4 Data sampling with a sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d9826-6384-462e-aa8a-a7c73cd6aad0",
   "metadata": {},
   "source": [
    "- Above, we took care of the tokenization (converting text into word tokens represented as token ID numbers)\n",
    "- Now, let's talk about how we create the data loading for LLMs\n",
    "- We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb44f4-0c43-4a6a-9c2f-9cf31452354c",
   "metadata": {},
   "source": [
    "<img src=\"figures/08.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9a3d50-885b-49bc-b791-9f5cc8bc7b7c",
   "metadata": {},
   "source": [
    "- For this, we use a sliding window approach, changing the position by +1:\n",
    "\n",
    "<img src=\"figures/09.png\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006212f-de45-468d-bdee-5806216d1679",
   "metadata": {},
   "source": [
    "- Note that in practice it's best to set the stride equal to the context length so that we don't have overlaps between the inputs (the targets are still shifted by +1 always)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb467e0-bdcd-4dda-b9b0-a738c5d33ac3",
   "metadata": {},
   "source": [
    "<img src=\"figures/10.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb55f51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "# creating a dataloader\n",
    "from supplementary import create_dataloader_v1\n",
    "\n",
    "# raw_text is the text that we read from the file \"the-verdict.txt\"\n",
    "# max_length is the maximum column length of the input tensor\n",
    "# stride is the number of positions to move the window by\n",
    "# batch_size is the number of rows in each batch (i.e here there are 8 rows in each batch)\n",
    "# shuffle is used to shuffle the data before creating the batches (here we have made shuffle as False)\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "# we have now prepared our data set for training of the LLM\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets) # The targets are the same as the inputs but shifted by one position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc671fb-6945-4594-b33f-8b462a69720d",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Exercise: Prepare your own favorite text dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
