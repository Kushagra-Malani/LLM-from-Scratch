{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0599d57c-16e4-478c-954d-89dbbd193ced",
   "metadata": {},
   "source": [
    "**LLM Workshop 2024 by Sebastian Raschka**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b82151-07c6-4867-b374-741258033b52",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 5) Loading pretrained weights (part 2; using LitGPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d617b8f-8493-4afa-8c91-f3d1ab79795b",
   "metadata": {},
   "source": [
    "- Now, we are loading the weights using an open-source library called LitGPT\n",
    "- LitGPT is fundamentally similar to the LLM code we implemented previously, but it is much more sophisticated and supports more than 20 different LLMs (Mistral, Gemma, Llama, Phi, and more)\n",
    "\n",
    "# ⚡ LitGPT\n",
    "\n",
    "**20+ high-performance LLMs with recipes to pretrain, finetune, deploy at scale.**\n",
    "\n",
    "<pre>\n",
    "✅ From scratch implementations     ✅ No abstractions    ✅ Beginner friendly   \n",
    "✅ Flash attention                  ✅ FSDP               ✅ LoRA, QLoRA, Adapter\n",
    "✅ Reduce GPU memory (fp4/8/16/32)  ✅ 1-1000+ GPUs/TPUs  ✅ 20+ LLMs            \n",
    "</pre>\n",
    "\n",
    "## Basic usage:\n",
    "\n",
    "```\n",
    "# ligpt [action] [model]\n",
    "litgpt  download  meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  chat      meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  evaluate  meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  finetune  meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  pretrain  meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  serve     meta-llama/Meta-Llama-3-8B-Instruct\n",
    "```\n",
    "\n",
    "\n",
    "- You can learn more about LitGPT in the [corresponding GitHub repository](https://github.com/Lightning-AI/litgpt), that contains many tutorials, use cases, and examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f9508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install litgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48cf71fa-af17-4c72-a6ab-f258a2b5a8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "litgpt version: 0.5.7\n",
      "torch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "# LitGPT helps use the GPT to use more sophisticated LLMs other than just gpt2 like phi-3, minstral, llama, etc. \n",
    "# It is not just a wrapper around the huggingface transformers library but an independent implementation optimized for performance & scalability\n",
    "# Lit-GPT can work with Hugging Face models by loading pre-trained weights\n",
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"litgpt\", \n",
    "        \"torch\",\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe29baa9-c3b0-493d-94b4-eaa8146d6b3c",
   "metadata": {},
   "source": [
    "- First, let's see what LLMs are supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae8df66-f391-4266-b437-a1f601a6ac40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uvloop is not installed. Falling back to the default asyncio event loop. Please install uvloop for better performance using `pip install uvloop`.\n",
      "uvloop is not installed. Falling back to the default asyncio event loop.\n",
      "Please specify --repo_id <repo_id>. Available values:\n",
      "allenai/OLMo-1B-hf\n",
      "allenai/OLMo-7B-hf\n",
      "allenai/OLMo-7B-Instruct-hf\n",
      "BSC-LT/salamandra-2b\n",
      "BSC-LT/salamandra-2b-instruct\n",
      "BSC-LT/salamandra-7b\n",
      "BSC-LT/salamandra-7b-instruct\n",
      "codellama/CodeLlama-13b-hf\n",
      "codellama/CodeLlama-13b-Instruct-hf\n",
      "codellama/CodeLlama-13b-Python-hf\n",
      "codellama/CodeLlama-34b-hf\n",
      "codellama/CodeLlama-34b-Instruct-hf\n",
      "codellama/CodeLlama-34b-Python-hf\n",
      "codellama/CodeLlama-70b-hf\n",
      "codellama/CodeLlama-70b-Instruct-hf\n",
      "codellama/CodeLlama-70b-Python-hf\n",
      "codellama/CodeLlama-7b-hf\n",
      "codellama/CodeLlama-7b-Instruct-hf\n",
      "codellama/CodeLlama-7b-Python-hf\n",
      "deepseek-ai/DeepSeek-R1-Distill-Llama-70B\n",
      "deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
      "EleutherAI/pythia-1.4b\n",
      "EleutherAI/pythia-1.4b-deduped\n",
      "EleutherAI/pythia-12b\n",
      "EleutherAI/pythia-12b-deduped\n",
      "EleutherAI/pythia-14m\n",
      "EleutherAI/pythia-160m\n",
      "EleutherAI/pythia-160m-deduped\n",
      "EleutherAI/pythia-1b\n",
      "EleutherAI/pythia-1b-deduped\n",
      "EleutherAI/pythia-2.8b\n",
      "EleutherAI/pythia-2.8b-deduped\n",
      "EleutherAI/pythia-31m\n",
      "EleutherAI/pythia-410m\n",
      "EleutherAI/pythia-410m-deduped\n",
      "EleutherAI/pythia-6.9b\n",
      "EleutherAI/pythia-6.9b-deduped\n",
      "EleutherAI/pythia-70m\n",
      "EleutherAI/pythia-70m-deduped\n",
      "garage-bAInd/Camel-Platypus2-13B\n",
      "garage-bAInd/Camel-Platypus2-70B\n",
      "garage-bAInd/Platypus-30B\n",
      "garage-bAInd/Platypus2-13B\n",
      "garage-bAInd/Platypus2-70B\n",
      "garage-bAInd/Platypus2-70B-instruct\n",
      "garage-bAInd/Platypus2-7B\n",
      "garage-bAInd/Stable-Platypus2-13B\n",
      "google/codegemma-7b-it\n",
      "google/gemma-2-27b\n",
      "google/gemma-2-27b-it\n",
      "google/gemma-2-2b\n",
      "google/gemma-2-2b-it\n",
      "google/gemma-2-9b\n",
      "google/gemma-2-9b-it\n",
      "google/gemma-2b\n",
      "google/gemma-2b-it\n",
      "google/gemma-7b\n",
      "google/gemma-7b-it\n",
      "HuggingFaceTB/SmolLM2-1.7B\n",
      "HuggingFaceTB/SmolLM2-1.7B-Instruct\n",
      "HuggingFaceTB/SmolLM2-135M\n",
      "HuggingFaceTB/SmolLM2-135M-Instruct\n",
      "HuggingFaceTB/SmolLM2-360M\n",
      "HuggingFaceTB/SmolLM2-360M-Instruct\n",
      "keeeeenw/MicroLlama\n",
      "meta-llama/Llama-2-13b-chat-hf\n",
      "meta-llama/Llama-2-13b-hf\n",
      "meta-llama/Llama-2-70b-chat-hf\n",
      "meta-llama/Llama-2-70b-hf\n",
      "meta-llama/Llama-2-7b-chat-hf\n",
      "meta-llama/Llama-2-7b-hf\n",
      "meta-llama/Llama-3.2-1B\n",
      "meta-llama/Llama-3.2-1B-Instruct\n",
      "meta-llama/Llama-3.2-3B\n",
      "meta-llama/Llama-3.2-3B-Instruct\n",
      "meta-llama/Llama-3.3-70B-Instruct\n",
      "meta-llama/Meta-Llama-3-70B\n",
      "meta-llama/Meta-Llama-3-70B-Instruct\n",
      "meta-llama/Meta-Llama-3-8B\n",
      "meta-llama/Meta-Llama-3-8B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-405B\n",
      "meta-llama/Meta-Llama-3.1-405B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-70B\n",
      "meta-llama/Meta-Llama-3.1-70B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-8B\n",
      "meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "microsoft/phi-1_5\n",
      "microsoft/phi-2\n",
      "microsoft/Phi-3-mini-128k-instruct\n",
      "microsoft/Phi-3-mini-4k-instruct\n",
      "microsoft/Phi-3.5-mini-instruct\n",
      "microsoft/phi-4\n",
      "mistralai/mathstral-7B-v0.1\n",
      "mistralai/Mistral-7B-Instruct-v0.1\n",
      "mistralai/Mistral-7B-Instruct-v0.2\n",
      "mistralai/Mistral-7B-Instruct-v0.3\n",
      "mistralai/Mistral-7B-v0.1\n",
      "mistralai/Mistral-7B-v0.3\n",
      "mistralai/Mistral-Large-Instruct-2407\n",
      "mistralai/Mistral-Large-Instruct-2411\n",
      "mistralai/Mixtral-8x22B-Instruct-v0.1\n",
      "mistralai/Mixtral-8x22B-v0.1\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "mistralai/Mixtral-8x7B-v0.1\n",
      "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF\n",
      "openlm-research/open_llama_13b\n",
      "openlm-research/open_llama_3b\n",
      "openlm-research/open_llama_7b\n",
      "Qwen/Qwen2.5-0.5B\n",
      "Qwen/Qwen2.5-0.5B-Instruct\n",
      "Qwen/Qwen2.5-1.5B\n",
      "Qwen/Qwen2.5-1.5B-Instruct\n",
      "Qwen/Qwen2.5-14B\n",
      "Qwen/Qwen2.5-14B-Instruct\n",
      "Qwen/Qwen2.5-32B\n",
      "Qwen/Qwen2.5-32B-Instruct\n",
      "Qwen/Qwen2.5-3B\n",
      "Qwen/Qwen2.5-3B-Instruct\n",
      "Qwen/Qwen2.5-72B\n",
      "Qwen/Qwen2.5-72B-Instruct\n",
      "Qwen/Qwen2.5-7B\n",
      "Qwen/Qwen2.5-7B-Instruct\n",
      "Qwen/Qwen2.5-Coder-0.5B\n",
      "Qwen/Qwen2.5-Coder-0.5B-Instruct\n",
      "Qwen/Qwen2.5-Coder-1.5B\n",
      "Qwen/Qwen2.5-Coder-1.5B-Instruct\n",
      "Qwen/Qwen2.5-Coder-14B\n",
      "Qwen/Qwen2.5-Coder-14B-Instruct\n",
      "Qwen/Qwen2.5-Coder-32B\n",
      "Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "Qwen/Qwen2.5-Coder-3B\n",
      "Qwen/Qwen2.5-Coder-3B-Instruct\n",
      "Qwen/Qwen2.5-Coder-7B\n",
      "Qwen/Qwen2.5-Coder-7B-Instruct\n",
      "Qwen/Qwen2.5-Math-1.5B\n",
      "Qwen/Qwen2.5-Math-1.5B-Instruct\n",
      "Qwen/Qwen2.5-Math-72B\n",
      "Qwen/Qwen2.5-Math-72B-Instruct\n",
      "Qwen/Qwen2.5-Math-7B\n",
      "Qwen/Qwen2.5-Math-7B-Instruct\n",
      "Qwen/QwQ-32B-Preview\n",
      "stabilityai/FreeWilly2\n",
      "stabilityai/stable-code-3b\n",
      "stabilityai/stablecode-completion-alpha-3b\n",
      "stabilityai/stablecode-completion-alpha-3b-4k\n",
      "stabilityai/stablecode-instruct-alpha-3b\n",
      "stabilityai/stablelm-3b-4e1t\n",
      "stabilityai/stablelm-base-alpha-3b\n",
      "stabilityai/stablelm-base-alpha-7b\n",
      "stabilityai/stablelm-tuned-alpha-3b\n",
      "stabilityai/stablelm-tuned-alpha-7b\n",
      "stabilityai/stablelm-zephyr-3b\n",
      "tiiuae/falcon-180B\n",
      "tiiuae/falcon-180B-chat\n",
      "tiiuae/falcon-40b\n",
      "tiiuae/falcon-40b-instruct\n",
      "tiiuae/falcon-7b\n",
      "tiiuae/falcon-7b-instruct\n",
      "tiiuae/Falcon3-10B-Base\n",
      "tiiuae/Falcon3-10B-Instruct\n",
      "tiiuae/Falcon3-1B-Base\n",
      "tiiuae/Falcon3-1B-Instruct\n",
      "tiiuae/Falcon3-3B-Base\n",
      "tiiuae/Falcon3-3B-Instruct\n",
      "tiiuae/Falcon3-7B-Base\n",
      "tiiuae/Falcon3-7B-Instruct\n",
      "TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\n",
      "togethercomputer/LLaMA-2-7B-32K\n",
      "Trelis/Llama-2-7b-chat-hf-function-calling-v2\n",
      "unsloth/Mistral-7B-v0.2\n"
     ]
    }
   ],
   "source": [
    "!litgpt download list\n",
    "# litgpt download list will show the available models implemented in LitGPT for download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2495037e-0068-49ad-9bed-0bcdc440727d",
   "metadata": {},
   "source": [
    "- We can then download an LLM via the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb0c202d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uvloop is not installed. Falling back to the default asyncio event loop. Please install uvloop for better performance using `pip install uvloop`."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing  0%|          | 00:00<?, ?it/s\n",
      "Loading weights: model.safetensors:   0%|          | 00:00<?, ?it/s\n",
      "Loading weights: model.safetensors:   0%|          | 00:00<00:45,  2.20it/s\n",
      "Loading weights: model.safetensors:   1%|          | 00:01<05:25,  3.27s/it\n",
      "Loading weights: model.safetensors:   1%|          | 00:03<06:36,  4.00s/it\n",
      "Loading weights: model.safetensors:   3%|▎         | 00:03<01:37,  1.00it/s\n",
      "Loading weights: model.safetensors:   3%|▎         | 00:03<01:27,  1.11it/s\n",
      "Loading weights: model.safetensors:   5%|▍         | 00:04<00:46,  2.03it/s\n",
      "Loading weights: model.safetensors:   6%|▌         | 00:04<00:41,  2.25it/s\n",
      "Loading weights: model.safetensors:   7%|▋         | 00:04<00:25,  3.59it/s\n",
      "Loading weights: model.safetensors:   8%|▊         | 00:04<00:24,  3.68it/s\n",
      "Loading weights: model.safetensors:  10%|▉         | 00:05<00:38,  2.34it/s\n",
      "Loading weights: model.safetensors:  10%|█         | 00:06<00:45,  1.99it/s\n",
      "Loading weights: model.safetensors:  12%|█▏        | 00:06<00:28,  3.13it/s\n",
      "Loading weights: model.safetensors:  14%|█▍        | 00:06<00:17,  4.84it/s\n",
      "Loading weights: model.safetensors:  15%|█▌        | 00:06<00:20,  4.15it/s\n",
      "Loading weights: model.safetensors:  17%|█▋        | 00:07<00:22,  3.64it/s\n",
      "Loading weights: model.safetensors:  18%|█▊        | 00:07<00:20,  4.08it/s\n",
      "Loading weights: model.safetensors:  19%|█▉        | 00:07<00:21,  3.71it/s\n",
      "Loading weights: model.safetensors:  20%|█▉        | 00:08<00:22,  3.62it/s\n",
      "Loading weights: model.safetensors:  21%|██▏       | 00:08<00:17,  4.51it/s\n",
      "Loading weights: model.safetensors:  22%|██▏       | 00:08<00:18,  4.15it/s\n",
      "Loading weights: model.safetensors:  23%|██▎       | 00:08<00:21,  3.56it/s\n",
      "Loading weights: model.safetensors:  23%|██▎       | 00:09<00:23,  3.33it/s\n",
      "Loading weights: model.safetensors:  24%|██▍       | 00:09<00:21,  3.49it/s\n",
      "Loading weights: model.safetensors:  24%|██▍       | 00:09<00:19,  3.83it/s\n",
      "Loading weights: model.safetensors:  26%|██▌       | 00:10<00:27,  2.69it/s\n",
      "Loading weights: model.safetensors:  27%|██▋       | 00:10<00:26,  2.75it/s\n",
      "Loading weights: model.safetensors:  28%|██▊       | 00:10<00:17,  4.14it/s\n",
      "Loading weights: model.safetensors:  29%|██▉       | 00:10<00:17,  4.03it/s\n",
      "Loading weights: model.safetensors:  31%|███       | 00:11<00:30,  2.24it/s\n",
      "Loading weights: model.safetensors:  31%|███▏      | 00:12<00:38,  1.80it/s\n",
      "Loading weights: model.safetensors:  33%|███▎      | 00:12<00:24,  2.69it/s\n",
      "Loading weights: model.safetensors:  34%|███▎      | 00:12<00:23,  2.78it/s\n",
      "Loading weights: model.safetensors:  35%|███▌      | 00:13<00:16,  3.93it/s\n",
      "Loading weights: model.safetensors:  36%|███▌      | 00:13<00:19,  3.21it/s\n",
      "Loading weights: model.safetensors:  38%|███▊      | 00:14<00:20,  3.05it/s\n",
      "Loading weights: model.safetensors:  38%|███▊      | 00:14<00:20,  3.07it/s\n",
      "Loading weights: model.safetensors:  40%|████      | 00:14<00:16,  3.72it/s\n",
      "Loading weights: model.safetensors:  41%|████      | 00:15<00:32,  1.84it/s\n",
      "Loading weights: model.safetensors:  41%|████▏     | 00:16<00:32,  1.79it/s\n",
      "Loading weights: model.safetensors:  43%|████▎     | 00:16<00:24,  2.37it/s\n",
      "Loading weights: model.safetensors:  43%|████▎     | 00:16<00:22,  2.50it/s\n",
      "Loading weights: model.safetensors:  45%|████▍     | 00:16<00:15,  3.56it/s\n",
      "Loading weights: model.safetensors:  45%|████▌     | 00:16<00:15,  3.53it/s\n",
      "Loading weights: model.safetensors:  47%|████▋     | 00:17<00:11,  4.59it/s\n",
      "Loading weights: model.safetensors:  48%|████▊     | 00:18<00:29,  1.77it/s\n",
      "Loading weights: model.safetensors:  48%|████▊     | 00:18<00:26,  1.94it/s\n",
      "Loading weights: model.safetensors:  50%|████▉     | 00:18<00:19,  2.64it/s\n",
      "Loading weights: model.safetensors:  50%|█████     | 00:18<00:18,  2.73it/s\n",
      "Loading weights: model.safetensors:  52%|█████▏    | 00:19<00:12,  3.96it/s\n",
      "Loading weights: model.safetensors:  52%|█████▏    | 00:19<00:13,  3.41it/s\n",
      "Loading weights: model.safetensors:  54%|█████▍    | 00:19<00:10,  4.44it/s\n",
      "Loading weights: model.safetensors:  55%|█████▍    | 00:19<00:10,  4.11it/s\n",
      "Loading weights: model.safetensors:  57%|█████▋    | 00:23<00:42,  1.01it/s\n",
      "Loading weights: model.safetensors:  57%|█████▋    | 00:24<00:45,  1.06s/it\n",
      "Loading weights: model.safetensors:  59%|█████▉    | 00:24<00:30,  1.36it/s\n",
      "Loading weights: model.safetensors:  60%|██████    | 00:24<00:23,  1.70it/s\n",
      "Loading weights: model.safetensors:  62%|██████▏   | 00:25<00:13,  2.86it/s\n",
      "Loading weights: model.safetensors:  65%|██████▍   | 00:25<00:08,  4.28it/s\n",
      "Loading weights: model.safetensors:  67%|██████▋   | 00:25<00:05,  5.92it/s\n",
      "Loading weights: model.safetensors:  70%|██████▉   | 00:25<00:03,  7.74it/s\n",
      "Loading weights: model.safetensors:  71%|███████▏  | 00:25<00:03,  9.04it/s\n",
      "Loading weights: model.safetensors: 100%|██████████| 00:27<00:00, 15.62it/s\n",
      "Loading weights: model.safetensors: 100%|██████████| 00:27<00:00,  3.67it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "uvloop is not installed. Falling back to the default asyncio event loop.\n",
      "Setting HF_HUB_ENABLE_HF_TRANSFER=1\n",
      "Converting checkpoint files to LitGPT format.\n",
      "{'checkpoint_dir': WindowsPath('checkpoints/microsoft/phi-1_5'),\n",
      " 'debug_mode': False,\n",
      " 'dtype': None,\n",
      " 'model_name': None}\n",
      "Saving converted checkpoint to checkpoints\\microsoft\\phi-1_5\n"
     ]
    }
   ],
   "source": [
    "# !litgpt download microsoft/phi-2\n",
    "!litgpt download microsoft/phi-1_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf5be0-4aa1-498f-b08a-68ff234cbea5",
   "metadata": {},
   "source": [
    "- And there's also a Python API to use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e057edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litgpt import LLM\n",
    "# if you download a new model then first do: del llm\n",
    "llm = LLM.load(\"microsoft/phi-1_5\")\n",
    "# lit_model.pth is just a LitGPT compatible weight file for the phi-1_5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8f04941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Llamas are herbivores and primarily feed on plants, grasses, and salt cacti. They also consume some insects. Their digestive systems are adapted to process plant material, allowing them to extract nutrients efficiently.\n",
      "\n",
      "Exercise 3\n"
     ]
    }
   ],
   "source": [
    "print(llm.generate(\"What do Llamas eat?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc775d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Llamas are herbivores that graze on grass, leaves, and other plants.\n",
      "\n",
      "(2). Llamas want to cross a dusty path to eat grass, but they have dark fur that blends in with the dunes. How can they avoid getting dirty?\n",
      "\n",
      "Answer: Llamas can walk towards the shade of the camels or moss-covered rocks to avoid the glare of the sun that causes the dust to reflect.\n",
      "\n",
      "(3). Llamas"
     ]
    }
   ],
   "source": [
    "# Generating text with one token at a time instead of generating the text all at once\n",
    "# This is useful for streaming the output to the console\n",
    "result = llm.generate(\"What do Llamas eat?\", stream=True, max_new_tokens=100)\n",
    "for e in result:\n",
    "    print(e, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288158da",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Exercise 2: Download an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2717b67f",
   "metadata": {},
   "source": [
    "- Download and try out an LLM of your own choice (recommendation: 7B parameters or smaller)\n",
    "- We will finetune the LLM in the next notebook\n",
    "- You can also try out the `litgpt chat` command from the terminal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
